{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from ppnp.pytorch import PPNP\n",
    "from ppnp.pytorch.training import train_model\n",
    "from ppnp.pytorch.earlystopping import stopping_args\n",
    "from ppnp.pytorch.propagation import PPRExact, PPRPowerIteration\n",
    "from ppnp.data.io import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format='%(asctime)s: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "First we need to load the dataset we want to train on. The datasets used are in the `SparseGraph` format. This is just a class providing the adjacency, attribute and label matrices in a dense (`np.ndarray`) or sparse (`scipy.sparse.csr_matrix`) matrix format and some (in principle unnecessary) convenience functions. If you want to use external datasets, you can e.g. use the `networkx_to_sparsegraph` method in `ppnp.data.io` for converting NetworkX graphs to our SparseGraph format.\n",
    "\n",
    "The four datasets from the paper (Cora-ML, Citeseer, PubMed and MS Academic) can be found in the directory `data`.\n",
    "\n",
    "For this example we choose the Cora-ML graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Undirected, unweighted and connected SparseGraph with 15962 edges (no self-loops). Data: adj_matrix (2810x2810), attr_matrix (2810x2879), labels (2810), node_names (2810), attr_names (2879), class_names (7)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_name = 'cora_ml'\n",
    "graph = load_dataset(graph_name)\n",
    "graph.standardize(select_lcc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up propagation\n",
    "\n",
    "Next we need to set up the proper propagation scheme. In the paper we've introduced the exact PPR propagation used in PPNP and the PPR power iteration propagation used in APPNP.\n",
    "\n",
    "Here we use the hyperparameters from the paper. Note that we should use a different `alpha = 0.2` for MS Academic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_ppnp = PPRExact(graph.adj_matrix, alpha=0.1)\n",
    "prop_appnp = PPRPowerIteration(graph.adj_matrix, alpha=0.1, niter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose model hyperparameters\n",
    "\n",
    "Now we choose the hyperparameters. These are the ones used in the paper for all datasets.\n",
    "\n",
    "Note that we choose the propagation for APPNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'hiddenunits': [64],\n",
    "    'drop_prob': 0.5,\n",
    "    'propagation': prop_appnp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split_args = {'ntrain_per_class': 20, 'nstopping': 500, 'nknown': 1500, 'seed': 2413340114}\n",
    "reg_lambda = 5e-3\n",
    "learning_rate = 0.01\n",
    "\n",
    "test = False\n",
    "device = 'cuda'\n",
    "print_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-08 10:18:46: PPNP: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': PPRPowerIteration()}\n",
      "2019-03-08 10:18:46: PyTorch seed: 1205423477\n",
      "2019-03-08 10:18:48: Epoch 0: Train loss = 2.00, train acc = 8.6, early stopping loss = 1.96, early stopping acc = 54.0 (0.081 sec)\n",
      "2019-03-08 10:18:50: Epoch 20: Train loss = 1.94, train acc = 70.7, early stopping loss = 1.95, early stopping acc = 51.2 (1.268 sec)\n",
      "2019-03-08 10:18:51: Epoch 40: Train loss = 1.90, train acc = 70.0, early stopping loss = 1.95, early stopping acc = 53.4 (1.222 sec)\n",
      "2019-03-08 10:18:52: Epoch 60: Train loss = 1.84, train acc = 77.1, early stopping loss = 1.93, early stopping acc = 59.8 (1.264 sec)\n",
      "2019-03-08 10:18:54: Epoch 80: Train loss = 1.75, train acc = 85.7, early stopping loss = 1.89, early stopping acc = 65.0 (1.274 sec)\n",
      "2019-03-08 10:18:55: Epoch 100: Train loss = 1.66, train acc = 90.7, early stopping loss = 1.84, early stopping acc = 69.8 (1.296 sec)\n",
      "2019-03-08 10:18:56: Epoch 120: Train loss = 1.59, train acc = 90.7, early stopping loss = 1.78, early stopping acc = 71.2 (1.281 sec)\n",
      "2019-03-08 10:18:57: Epoch 140: Train loss = 1.48, train acc = 94.3, early stopping loss = 1.71, early stopping acc = 75.0 (1.285 sec)\n",
      "2019-03-08 10:18:59: Epoch 160: Train loss = 1.42, train acc = 93.6, early stopping loss = 1.66, early stopping acc = 76.4 (1.276 sec)\n",
      "2019-03-08 10:19:00: Epoch 180: Train loss = 1.34, train acc = 97.1, early stopping loss = 1.61, early stopping acc = 78.4 (1.286 sec)\n",
      "2019-03-08 10:19:01: Epoch 200: Train loss = 1.28, train acc = 96.4, early stopping loss = 1.56, early stopping acc = 78.6 (1.321 sec)\n",
      "2019-03-08 10:19:03: Epoch 220: Train loss = 1.24, train acc = 97.9, early stopping loss = 1.52, early stopping acc = 79.8 (1.281 sec)\n",
      "2019-03-08 10:19:04: Epoch 240: Train loss = 1.19, train acc = 97.9, early stopping loss = 1.48, early stopping acc = 79.2 (1.281 sec)\n",
      "2019-03-08 10:19:05: Epoch 260: Train loss = 1.13, train acc = 98.6, early stopping loss = 1.44, early stopping acc = 79.8 (1.278 sec)\n",
      "2019-03-08 10:19:06: Epoch 280: Train loss = 1.10, train acc = 97.1, early stopping loss = 1.41, early stopping acc = 80.0 (1.305 sec)\n",
      "2019-03-08 10:19:08: Epoch 300: Train loss = 1.04, train acc = 98.6, early stopping loss = 1.38, early stopping acc = 80.6 (1.284 sec)\n",
      "2019-03-08 10:19:09: Epoch 320: Train loss = 1.02, train acc = 98.6, early stopping loss = 1.35, early stopping acc = 80.8 (1.279 sec)\n",
      "2019-03-08 10:19:10: Epoch 340: Train loss = 1.02, train acc = 98.6, early stopping loss = 1.34, early stopping acc = 80.6 (1.276 sec)\n",
      "2019-03-08 10:19:12: Epoch 360: Train loss = 0.96, train acc = 97.1, early stopping loss = 1.30, early stopping acc = 81.2 (1.281 sec)\n",
      "2019-03-08 10:19:13: Epoch 380: Train loss = 0.94, train acc = 97.9, early stopping loss = 1.28, early stopping acc = 81.4 (1.300 sec)\n",
      "2019-03-08 10:19:14: Epoch 400: Train loss = 0.89, train acc = 97.9, early stopping loss = 1.26, early stopping acc = 80.4 (1.283 sec)\n",
      "2019-03-08 10:19:15: Epoch 420: Train loss = 0.90, train acc = 97.9, early stopping loss = 1.24, early stopping acc = 82.6 (1.289 sec)\n",
      "2019-03-08 10:19:17: Epoch 440: Train loss = 0.87, train acc = 100.0, early stopping loss = 1.23, early stopping acc = 80.4 (1.286 sec)\n",
      "2019-03-08 10:19:18: Epoch 460: Train loss = 0.85, train acc = 100.0, early stopping loss = 1.21, early stopping acc = 81.6 (1.304 sec)\n",
      "2019-03-08 10:19:19: Epoch 480: Train loss = 0.84, train acc = 98.6, early stopping loss = 1.19, early stopping acc = 80.6 (1.288 sec)\n",
      "2019-03-08 10:19:21: Epoch 500: Train loss = 0.84, train acc = 98.6, early stopping loss = 1.18, early stopping acc = 81.6 (1.284 sec)\n",
      "2019-03-08 10:19:22: Epoch 520: Train loss = 0.80, train acc = 97.9, early stopping loss = 1.16, early stopping acc = 80.8 (1.276 sec)\n",
      "2019-03-08 10:19:23: Epoch 540: Train loss = 0.81, train acc = 98.6, early stopping loss = 1.15, early stopping acc = 82.2 (1.295 sec)\n",
      "2019-03-08 10:19:24: Epoch 560: Train loss = 0.75, train acc = 100.0, early stopping loss = 1.16, early stopping acc = 80.6 (1.269 sec)\n",
      "2019-03-08 10:19:26: Epoch 580: Train loss = 0.76, train acc = 98.6, early stopping loss = 1.13, early stopping acc = 82.0 (1.276 sec)\n",
      "2019-03-08 10:19:27: Epoch 600: Train loss = 0.75, train acc = 100.0, early stopping loss = 1.12, early stopping acc = 81.2 (1.280 sec)\n",
      "2019-03-08 10:19:28: Epoch 620: Train loss = 0.74, train acc = 98.6, early stopping loss = 1.12, early stopping acc = 80.4 (1.275 sec)\n",
      "2019-03-08 10:19:30: Epoch 640: Train loss = 0.74, train acc = 99.3, early stopping loss = 1.11, early stopping acc = 81.2 (1.293 sec)\n",
      "2019-03-08 10:19:31: Epoch 660: Train loss = 0.73, train acc = 98.6, early stopping loss = 1.09, early stopping acc = 81.2 (1.272 sec)\n",
      "2019-03-08 10:19:32: Epoch 680: Train loss = 0.72, train acc = 99.3, early stopping loss = 1.08, early stopping acc = 82.8 (1.282 sec)\n",
      "2019-03-08 10:19:33: Epoch 700: Train loss = 0.70, train acc = 97.9, early stopping loss = 1.07, early stopping acc = 80.4 (1.279 sec)\n",
      "2019-03-08 10:19:35: Epoch 720: Train loss = 0.68, train acc = 100.0, early stopping loss = 1.06, early stopping acc = 82.2 (1.305 sec)\n",
      "2019-03-08 10:19:36: Epoch 740: Train loss = 0.69, train acc = 100.0, early stopping loss = 1.06, early stopping acc = 82.8 (1.281 sec)\n",
      "2019-03-08 10:19:37: Epoch 760: Train loss = 0.66, train acc = 100.0, early stopping loss = 1.06, early stopping acc = 81.0 (1.274 sec)\n",
      "2019-03-08 10:19:38: Epoch 780: Train loss = 0.63, train acc = 99.3, early stopping loss = 1.04, early stopping acc = 84.0 (1.282 sec)\n",
      "2019-03-08 10:19:40: Epoch 800: Train loss = 0.63, train acc = 100.0, early stopping loss = 1.03, early stopping acc = 81.4 (1.279 sec)\n",
      "2019-03-08 10:19:41: Epoch 820: Train loss = 0.62, train acc = 100.0, early stopping loss = 1.02, early stopping acc = 82.4 (1.295 sec)\n",
      "2019-03-08 10:19:42: Epoch 840: Train loss = 0.60, train acc = 100.0, early stopping loss = 1.02, early stopping acc = 81.8 (1.281 sec)\n",
      "2019-03-08 10:19:44: Epoch 860: Train loss = 0.62, train acc = 100.0, early stopping loss = 1.02, early stopping acc = 82.2 (1.271 sec)\n",
      "2019-03-08 10:19:45: Epoch 880: Train loss = 0.59, train acc = 99.3, early stopping loss = 1.01, early stopping acc = 81.0 (1.269 sec)\n",
      "2019-03-08 10:19:46: Epoch 900: Train loss = 0.58, train acc = 100.0, early stopping loss = 0.99, early stopping acc = 81.6 (1.275 sec)\n",
      "2019-03-08 10:19:47: Epoch 920: Train loss = 0.59, train acc = 100.0, early stopping loss = 0.99, early stopping acc = 81.2 (1.271 sec)\n",
      "2019-03-08 10:19:49: Epoch 940: Train loss = 0.59, train acc = 100.0, early stopping loss = 0.99, early stopping acc = 82.0 (1.265 sec)\n",
      "2019-03-08 10:19:50: Epoch 960: Train loss = 0.60, train acc = 99.3, early stopping loss = 0.99, early stopping acc = 82.6 (1.269 sec)\n",
      "2019-03-08 10:19:51: Epoch 980: Train loss = 0.56, train acc = 99.3, early stopping loss = 0.98, early stopping acc = 81.8 (1.252 sec)\n",
      "2019-03-08 10:19:52: Epoch 1000: Train loss = 0.58, train acc = 99.3, early stopping loss = 0.96, early stopping acc = 83.2 (1.275 sec)\n",
      "2019-03-08 10:19:54: Epoch 1020: Train loss = 0.54, train acc = 100.0, early stopping loss = 0.97, early stopping acc = 82.6 (1.261 sec)\n",
      "2019-03-08 10:19:55: Epoch 1040: Train loss = 0.55, train acc = 100.0, early stopping loss = 0.97, early stopping acc = 81.2 (1.263 sec)\n",
      "2019-03-08 10:19:56: Epoch 1060: Train loss = 0.52, train acc = 100.0, early stopping loss = 0.97, early stopping acc = 82.2 (1.255 sec)\n",
      "2019-03-08 10:19:58: Epoch 1080: Train loss = 0.56, train acc = 99.3, early stopping loss = 0.97, early stopping acc = 80.8 (1.277 sec)\n",
      "2019-03-08 10:19:59: Epoch 1100: Train loss = 0.54, train acc = 100.0, early stopping loss = 0.95, early stopping acc = 81.6 (1.314 sec)\n",
      "2019-03-08 10:20:00: Epoch 1120: Train loss = 0.55, train acc = 99.3, early stopping loss = 0.93, early stopping acc = 81.8 (1.458 sec)\n",
      "2019-03-08 10:20:02: Epoch 1140: Train loss = 0.51, train acc = 100.0, early stopping loss = 0.95, early stopping acc = 81.4 (1.455 sec)\n",
      "2019-03-08 10:20:03: Epoch 1160: Train loss = 0.53, train acc = 100.0, early stopping loss = 0.95, early stopping acc = 81.2 (1.462 sec)\n",
      "2019-03-08 10:20:05: Epoch 1180: Train loss = 0.52, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 82.2 (1.491 sec)\n",
      "2019-03-08 10:20:06: Epoch 1200: Train loss = 0.53, train acc = 100.0, early stopping loss = 0.93, early stopping acc = 81.6 (1.455 sec)\n",
      "2019-03-08 10:20:08: Epoch 1220: Train loss = 0.52, train acc = 100.0, early stopping loss = 0.97, early stopping acc = 79.6 (1.401 sec)\n",
      "2019-03-08 10:20:09: Epoch 1240: Train loss = 0.48, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 80.8 (1.353 sec)\n",
      "2019-03-08 10:20:10: Epoch 1260: Train loss = 0.51, train acc = 100.0, early stopping loss = 0.96, early stopping acc = 80.6 (1.408 sec)\n",
      "2019-03-08 10:20:12: Epoch 1280: Train loss = 0.48, train acc = 100.0, early stopping loss = 0.92, early stopping acc = 81.4 (1.365 sec)\n",
      "2019-03-08 10:20:13: Epoch 1300: Train loss = 0.48, train acc = 100.0, early stopping loss = 0.89, early stopping acc = 82.8 (1.376 sec)\n",
      "2019-03-08 10:20:14: Epoch 1320: Train loss = 0.48, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 81.6 (1.363 sec)\n",
      "2019-03-08 10:20:16: Epoch 1340: Train loss = 0.47, train acc = 100.0, early stopping loss = 0.90, early stopping acc = 82.2 (1.343 sec)\n",
      "2019-03-08 10:20:17: Epoch 1360: Train loss = 0.49, train acc = 99.3, early stopping loss = 0.93, early stopping acc = 80.4 (1.302 sec)\n",
      "2019-03-08 10:20:18: Epoch 1380: Train loss = 0.48, train acc = 100.0, early stopping loss = 0.89, early stopping acc = 81.2 (1.276 sec)\n",
      "2019-03-08 10:20:20: Epoch 1400: Train loss = 0.46, train acc = 99.3, early stopping loss = 0.89, early stopping acc = 82.0 (1.274 sec)\n",
      "2019-03-08 10:20:21: Epoch 1420: Train loss = 0.45, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 82.4 (1.271 sec)\n",
      "2019-03-08 10:20:22: Epoch 1440: Train loss = 0.46, train acc = 99.3, early stopping loss = 0.88, early stopping acc = 81.4 (1.294 sec)\n",
      "2019-03-08 10:20:23: Epoch 1460: Train loss = 0.46, train acc = 99.3, early stopping loss = 0.90, early stopping acc = 80.8 (1.273 sec)\n",
      "2019-03-08 10:20:25: Epoch 1480: Train loss = 0.45, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 80.8 (1.267 sec)\n",
      "2019-03-08 10:20:26: Epoch 1500: Train loss = 0.44, train acc = 100.0, early stopping loss = 0.89, early stopping acc = 82.0 (1.255 sec)\n",
      "2019-03-08 10:20:27: Epoch 1520: Train loss = 0.43, train acc = 99.3, early stopping loss = 0.90, early stopping acc = 81.2 (1.295 sec)\n",
      "2019-03-08 10:20:29: Epoch 1540: Train loss = 0.44, train acc = 99.3, early stopping loss = 0.86, early stopping acc = 81.6 (1.261 sec)\n",
      "2019-03-08 10:20:30: Epoch 1560: Train loss = 0.43, train acc = 100.0, early stopping loss = 0.87, early stopping acc = 81.4 (1.261 sec)\n",
      "2019-03-08 10:20:31: Epoch 1580: Train loss = 0.43, train acc = 99.3, early stopping loss = 0.89, early stopping acc = 81.2 (1.261 sec)\n",
      "2019-03-08 10:20:32: Epoch 1600: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.86, early stopping acc = 81.2 (1.259 sec)\n",
      "2019-03-08 10:20:34: Epoch 1620: Train loss = 0.44, train acc = 100.0, early stopping loss = 0.88, early stopping acc = 80.8 (1.332 sec)\n",
      "2019-03-08 10:20:35: Epoch 1640: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 83.4 (1.459 sec)\n",
      "2019-03-08 10:20:37: Epoch 1660: Train loss = 0.44, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 84.2 (1.451 sec)\n",
      "2019-03-08 10:20:38: Epoch 1680: Train loss = 0.41, train acc = 98.6, early stopping loss = 0.88, early stopping acc = 80.2 (1.452 sec)\n",
      "2019-03-08 10:20:40: Epoch 1700: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.87, early stopping acc = 81.6 (1.484 sec)\n",
      "2019-03-08 10:20:41: Epoch 1720: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 84.4 (1.455 sec)\n",
      "2019-03-08 10:20:42: Epoch 1740: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.86, early stopping acc = 82.0 (1.456 sec)\n",
      "2019-03-08 10:20:44: Epoch 1760: Train loss = 0.41, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 83.2 (1.449 sec)\n",
      "2019-03-08 10:20:45: Epoch 1780: Train loss = 0.42, train acc = 99.3, early stopping loss = 0.83, early stopping acc = 82.4 (1.449 sec)\n",
      "2019-03-08 10:20:47: Epoch 1800: Train loss = 0.41, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 81.6 (1.471 sec)\n",
      "2019-03-08 10:20:48: Epoch 1820: Train loss = 0.41, train acc = 99.3, early stopping loss = 0.87, early stopping acc = 79.6 (1.470 sec)\n",
      "2019-03-08 10:20:50: Epoch 1840: Train loss = 0.40, train acc = 99.3, early stopping loss = 0.83, early stopping acc = 83.2 (1.450 sec)\n",
      "2019-03-08 10:20:51: Epoch 1860: Train loss = 0.42, train acc = 99.3, early stopping loss = 0.84, early stopping acc = 82.4 (1.451 sec)\n",
      "2019-03-08 10:20:53: Epoch 1880: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.81, early stopping acc = 84.0 (1.496 sec)\n",
      "2019-03-08 10:20:54: Epoch 1900: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 81.6 (1.462 sec)\n",
      "2019-03-08 10:20:56: Epoch 1920: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.83, early stopping acc = 82.8 (1.456 sec)\n",
      "2019-03-08 10:20:57: Epoch 1940: Train loss = 0.39, train acc = 98.6, early stopping loss = 0.82, early stopping acc = 83.8 (1.464 sec)\n",
      "2019-03-08 10:20:58: Epoch 1960: Train loss = 0.38, train acc = 99.3, early stopping loss = 0.85, early stopping acc = 80.6 (1.451 sec)\n",
      "2019-03-08 10:21:00: Epoch 1980: Train loss = 0.40, train acc = 100.0, early stopping loss = 0.81, early stopping acc = 81.2 (1.482 sec)\n",
      "2019-03-08 10:21:02: Epoch 2000: Train loss = 0.39, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 80.6 (1.532 sec)\n",
      "2019-03-08 10:21:03: Epoch 2020: Train loss = 0.39, train acc = 98.6, early stopping loss = 0.85, early stopping acc = 82.0 (1.467 sec)\n",
      "2019-03-08 10:21:04: Epoch 2040: Train loss = 0.38, train acc = 99.3, early stopping loss = 0.82, early stopping acc = 82.8 (1.452 sec)\n",
      "2019-03-08 10:21:06: Epoch 2060: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.80, early stopping acc = 83.2 (1.486 sec)\n",
      "2019-03-08 10:21:07: Last epoch: 2071, best epoch: 1971 (138.322 sec)\n",
      "2019-03-08 10:21:07: Early stopping accuracy: 84.8%\n",
      "2019-03-08 10:21:07: Validation accuracy: 84.7%\n"
     ]
    }
   ],
   "source": [
    "model, result = train_model(\n",
    "        graph_name, PPNP, graph, model_args, learning_rate, reg_lambda,\n",
    "        idx_split_args, stopping_args, test, device, None, print_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
